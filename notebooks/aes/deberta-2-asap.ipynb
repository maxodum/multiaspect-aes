{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T20:59:41.702780Z",
     "iopub.status.busy": "2025-05-16T20:59:41.702537Z",
     "iopub.status.idle": "2025-05-16T21:00:57.946916Z",
     "shell.execute_reply": "2025-05-16T21:00:57.945705Z",
     "shell.execute_reply.started": "2025-05-16T20:59:41.702754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch datasets scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:03:38.017093Z",
     "iopub.status.busy": "2025-05-16T21:03:38.016246Z",
     "iopub.status.idle": "2025-05-16T21:03:50.174026Z",
     "shell.execute_reply": "2025-05-16T21:03:50.173269Z",
     "shell.execute_reply.started": "2025-05-16T21:03:38.017031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:03:57.646160Z",
     "iopub.status.busy": "2025-05-16T21:03:57.645635Z",
     "iopub.status.idle": "2025-05-16T21:03:57.969043Z",
     "shell.execute_reply": "2025-05-16T21:03:57.968272Z",
     "shell.execute_reply.started": "2025-05-16T21:03:57.646135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/kaggle/input/dress-d/train.csv', index_col=0)\n",
    "val_df = pd.read_csv('/kaggle/input/dress-d/val.csv', index_col=0)\n",
    "test_df = pd.read_csv('/kaggle/input/dress-d/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:00.322817Z",
     "iopub.status.busy": "2025-05-16T21:04:00.322516Z",
     "iopub.status.idle": "2025-05-16T21:04:00.328844Z",
     "shell.execute_reply": "2025-05-16T21:04:00.328114Z",
     "shell.execute_reply.started": "2025-05-16T21:04:00.322794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EllipseDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"full_text\"].to_list()\n",
    "\n",
    "\n",
    "\n",
    "        scores = torch.tensor(row[[\"Overall\",\n",
    "                                   \"Cohesion\",\n",
    "                                   \"Syntax\",\n",
    "                                   \"Vocabulary\",\n",
    "                                   \"Phraseology\",\n",
    "                                   \"Grammar\",\n",
    "                                   \"Conventions\"]\n",
    "                                  ].values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "        inputs = self.tokenizer(text,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                                max_length=512)\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"scores\": scores\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:08.469967Z",
     "iopub.status.busy": "2025-05-16T21:04:08.469691Z",
     "iopub.status.idle": "2025-05-16T21:04:10.078329Z",
     "shell.execute_reply": "2025-05-16T21:04:10.077736Z",
     "shell.execute_reply.started": "2025-05-16T21:04:08.469947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29c28f5144a4509b042d93c8fe231a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe40decbfc104bab9f197fc51cca2631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eefbd006c254472a25c6216d48ea941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=False)\n",
    "special_token = \"\\n\\n\"\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [special_token]})\n",
    "\n",
    "train_dataset = EllipseDataset(train_df.reset_index(drop=True), tokenizer)\n",
    "val_dataset = EllipseDataset(val_df.reset_index(drop=True), tokenizer)\n",
    "test_dataset = EllipseDataset(test_df.reset_index(drop=True), tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:11.861113Z",
     "iopub.status.busy": "2025-05-16T21:04:11.860299Z",
     "iopub.status.idle": "2025-05-16T21:04:11.868768Z",
     "shell.execute_reply": "2025-05-16T21:04:11.867913Z",
     "shell.execute_reply.started": "2025-05-16T21:04:11.861063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DebertaNHeads(nn.Module):\n",
    "    def __init__(self, model_name=\"microsoft/deberta-v3-base\", num_aspects=7):\n",
    "        super().__init__()\n",
    "        self.deberta = AutoModel.from_pretrained(model_name)\n",
    "        self.deberta.resize_token_embeddings(len(tokenizer))\n",
    "        hidden_size = self.deberta.config.hidden_size\n",
    "\n",
    "        self.num_aspects = num_aspects\n",
    "        \n",
    "        self.regression_heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, 1) for _ in range(num_aspects)\n",
    "        ])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for head in self.regression_heads:\n",
    "            init.xavier_uniform_(head.weight)\n",
    "            if head.bias is not None:\n",
    "                init.zeros_(head.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        regression_outputs = []\n",
    "\n",
    "        for i in range(self.num_aspects):\n",
    "            regression_outputs.append(self.sigmoid(self.regression_heads[i](pooled_output)))\n",
    "\n",
    "        regression_outputs = torch.cat(regression_outputs, dim=1)\n",
    "\n",
    "        return regression_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:13.750184Z",
     "iopub.status.busy": "2025-05-16T21:04:13.749569Z",
     "iopub.status.idle": "2025-05-16T21:04:35.141573Z",
     "shell.execute_reply": "2025-05-16T21:04:35.140640Z",
     "shell.execute_reply.started": "2025-05-16T21:04:13.750161Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 21:04:20.267744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747429460.457499      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747429460.516181      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5e77ff9e0448fc97f31907f926d16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff32996229f44f00b3a3fc6c49c127b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DebertaNHeads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:41.626926Z",
     "iopub.status.busy": "2025-05-16T21:04:41.625883Z",
     "iopub.status.idle": "2025-05-16T21:04:41.662040Z",
     "shell.execute_reply": "2025-05-16T21:04:41.661277Z",
     "shell.execute_reply.started": "2025-05-16T21:04:41.626900Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:43.259839Z",
     "iopub.status.busy": "2025-05-16T21:04:43.259201Z",
     "iopub.status.idle": "2025-05-16T21:04:45.051645Z",
     "shell.execute_reply": "2025-05-16T21:04:45.050931Z",
     "shell.execute_reply.started": "2025-05-16T21:04:43.259817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "num_training_steps = len(train_loader) * 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:56.560897Z",
     "iopub.status.busy": "2025-05-16T21:04:56.560336Z",
     "iopub.status.idle": "2025-05-16T21:04:56.564544Z",
     "shell.execute_reply": "2025-05-16T21:04:56.563929Z",
     "shell.execute_reply.started": "2025-05-16T21:04:56.560872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def combined_loss(reg_preds, reg_labels):\n",
    "    regression_loss = nn.BCELoss()\n",
    "    total_reg_loss = 0\n",
    "    total_reg_loss += regression_loss(reg_preds, reg_labels)\n",
    "\n",
    "    return total_reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:57.014095Z",
     "iopub.status.busy": "2025-05-16T21:04:57.013466Z",
     "iopub.status.idle": "2025-05-16T21:04:57.019134Z",
     "shell.execute_reply": "2025-05-16T21:04:57.018336Z",
     "shell.execute_reply.started": "2025-05-16T21:04:57.014053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_qwk_for_aspects(class_preds, class_labels, aspects=['Overall', 'Cohesion', 'Syntax', 'Vocabulary', 'Phraseology', 'Grammar', 'Conventions']):\n",
    "    class_preds = class_preds * 4 + 1\n",
    "    class_labels = class_labels * 4 + 1\n",
    "\n",
    "    class_preds = np.round(class_preds * 2) / 2\n",
    "    class_labels = np.round(class_labels * 2) / 2\n",
    "\n",
    "    class_preds = ((class_preds - 1) * 2).astype(int)\n",
    "    class_labels = ((class_labels - 1) * 2).astype(int)\n",
    "\n",
    "    qwk_scores = {}\n",
    "    for i, aspect in enumerate(aspects):\n",
    "        qwk_score = cohen_kappa_score(class_preds[:, i], class_labels[:, i], weights='quadratic')\n",
    "        qwk_scores[aspect] = qwk_score\n",
    "        print(f'QWK for {aspect}: {qwk_score:.4f}')\n",
    "\n",
    "    return qwk_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:58.320200Z",
     "iopub.status.busy": "2025-05-16T21:04:58.319895Z",
     "iopub.status.idle": "2025-05-16T21:04:58.325190Z",
     "shell.execute_reply": "2025-05-16T21:04:58.324344Z",
     "shell.execute_reply.started": "2025-05-16T21:04:58.320178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for layer in model.deberta.encoder.layer[:9]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "for param in model.deberta.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:04:59.510463Z",
     "iopub.status.busy": "2025-05-16T21:04:59.509759Z",
     "iopub.status.idle": "2025-05-16T21:50:49.313142Z",
     "shell.execute_reply": "2025-05-16T21:50:49.312330Z",
     "shell.execute_reply.started": "2025-05-16T21:04:59.510442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 501/501 [04:03<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6905017965092155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 163/163 [00:32<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7599\n",
      "QWK for Cohesion: 0.6261\n",
      "QWK for Syntax: 0.6972\n",
      "QWK for Vocabulary: 0.6632\n",
      "QWK for Phraseology: 0.6818\n",
      "QWK for Grammar: 0.7204\n",
      "QWK for Conventions: 0.7046\n",
      "New best QWK: 0.6933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 501/501 [04:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6741249481599012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7574\n",
      "QWK for Cohesion: 0.6232\n",
      "QWK for Syntax: 0.6793\n",
      "QWK for Vocabulary: 0.6208\n",
      "QWK for Phraseology: 0.6652\n",
      "QWK for Grammar: 0.7014\n",
      "QWK for Conventions: 0.6988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 501/501 [04:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6665480284395808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7433\n",
      "QWK for Cohesion: 0.6465\n",
      "QWK for Syntax: 0.6939\n",
      "QWK for Vocabulary: 0.6255\n",
      "QWK for Phraseology: 0.7052\n",
      "QWK for Grammar: 0.7130\n",
      "QWK for Conventions: 0.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 501/501 [04:02<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6602743283479275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7646\n",
      "QWK for Cohesion: 0.6399\n",
      "QWK for Syntax: 0.7055\n",
      "QWK for Vocabulary: 0.6709\n",
      "QWK for Phraseology: 0.6877\n",
      "QWK for Grammar: 0.7199\n",
      "QWK for Conventions: 0.7113\n",
      "New best QWK: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 501/501 [04:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6544264995766257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 163/163 [00:32<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7361\n",
      "QWK for Cohesion: 0.6109\n",
      "QWK for Syntax: 0.6733\n",
      "QWK for Vocabulary: 0.5892\n",
      "QWK for Phraseology: 0.6658\n",
      "QWK for Grammar: 0.6963\n",
      "QWK for Conventions: 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 501/501 [04:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6494898661049064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 163/163 [00:32<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7407\n",
      "QWK for Cohesion: 0.6197\n",
      "QWK for Syntax: 0.6901\n",
      "QWK for Vocabulary: 0.6455\n",
      "QWK for Phraseology: 0.6638\n",
      "QWK for Grammar: 0.6988\n",
      "QWK for Conventions: 0.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 501/501 [04:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6454349118554426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 163/163 [00:32<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7491\n",
      "QWK for Cohesion: 0.6342\n",
      "QWK for Syntax: 0.6843\n",
      "QWK for Vocabulary: 0.6574\n",
      "QWK for Phraseology: 0.6825\n",
      "QWK for Grammar: 0.7184\n",
      "QWK for Conventions: 0.6906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 501/501 [04:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6435613672652406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7470\n",
      "QWK for Cohesion: 0.6360\n",
      "QWK for Syntax: 0.6824\n",
      "QWK for Vocabulary: 0.6592\n",
      "QWK for Phraseology: 0.6946\n",
      "QWK for Grammar: 0.7161\n",
      "QWK for Conventions: 0.6818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 501/501 [04:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6424735938003677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7543\n",
      "QWK for Cohesion: 0.6346\n",
      "QWK for Syntax: 0.6838\n",
      "QWK for Vocabulary: 0.6578\n",
      "QWK for Phraseology: 0.6960\n",
      "QWK for Grammar: 0.7172\n",
      "QWK for Conventions: 0.6995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 501/501 [04:01<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 0.6418111404615962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7473\n",
      "QWK for Cohesion: 0.6314\n",
      "QWK for Syntax: 0.6887\n",
      "QWK for Vocabulary: 0.6606\n",
      "QWK for Phraseology: 0.6933\n",
      "QWK for Grammar: 0.7172\n",
      "QWK for Conventions: 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "qwk_scores = []\n",
    "best_qwk = -float('inf')\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    loss_per_epoch = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        reg_target = batch[\"scores\"].to(device)\n",
    "        reg_preds = model(input_ids, attention_mask)\n",
    "        loss = 0\n",
    "        for i in range(7):\n",
    "            loss += combined_loss(reg_preds[:, i], reg_target[:, i])\n",
    "        loss /= 7\n",
    "        loss_per_epoch += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = loss_per_epoch / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f'Loss_per_epoch = {avg_loss}')\n",
    "\n",
    "    model.eval()\n",
    "    loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    with torch.no_grad():\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            reg_target = batch[\"scores\"].to(device)\n",
    "            reg_preds = model(input_ids, attention_mask)\n",
    "            all_preds.append(reg_preds)\n",
    "            all_targets.append(reg_target)\n",
    "    all_preds = torch.cat(all_preds, dim=0).detach().cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets, dim=0).detach().cpu().numpy()\n",
    "    qwk = evaluate_qwk_for_aspects(all_preds, all_targets, aspects=['Overall', 'Cohesion', 'Syntax', 'Vocabulary', 'Phraseology', 'Grammar', 'Conventions'])\n",
    "    qwk_scores.append(qwk)\n",
    "    avg_qwk = sum(qwk.values()) / len(qwk)\n",
    "    if avg_qwk - best_qwk > 1e-6:\n",
    "        best_qwk = avg_qwk\n",
    "        print(f\"New best QWK: {best_qwk:.4f}\")\n",
    "        torch.save(model.state_dict(), \"/kaggle/working/best_first_model.pth\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:50:59.504855Z",
     "iopub.status.busy": "2025-05-16T21:50:59.504582Z",
     "iopub.status.idle": "2025-05-16T21:51:02.299741Z",
     "shell.execute_reply": "2025-05-16T21:51:02.298915Z",
     "shell.execute_reply.started": "2025-05-16T21:50:59.504835Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaNHeads(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128002, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (regression_heads): ModuleList(\n",
       "    (0-6): 7 x Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DebertaNHeads()\n",
    "model.load_state_dict(torch.load('/kaggle/working/best_first_model.pth', weights_only=False))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:51:04.057734Z",
     "iopub.status.busy": "2025-05-16T21:51:04.057131Z",
     "iopub.status.idle": "2025-05-16T21:51:36.548863Z",
     "shell.execute_reply": "2025-05-16T21:51:36.548132Z",
     "shell.execute_reply.started": "2025-05-16T21:51:04.057710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:32<00:00,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7382\n",
      "QWK for Cohesion: 0.6244\n",
      "QWK for Syntax: 0.6650\n",
      "QWK for Vocabulary: 0.6316\n",
      "QWK for Phraseology: 0.6425\n",
      "QWK for Grammar: 0.6955\n",
      "QWK for Conventions: 0.6918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "loop = tqdm(test_loader)\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        target = batch[\"scores\"].to(device)\n",
    "        preds = model(input_ids, attention_mask)\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(target)\n",
    "all_preds = torch.cat(all_preds, dim=0).detach().cpu().numpy()\n",
    "all_targets = torch.cat(all_targets, dim=0).detach().cpu().numpy()\n",
    "qwk = evaluate_qwk_for_aspects(all_preds, all_targets, aspects=['Overall', 'Cohesion', 'Syntax', 'Vocabulary', 'Phraseology', 'Grammar', 'Conventions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:51:49.108340Z",
     "iopub.status.busy": "2025-05-16T21:51:49.107730Z",
     "iopub.status.idle": "2025-05-16T21:51:49.463859Z",
     "shell.execute_reply": "2025-05-16T21:51:49.463316Z",
     "shell.execute_reply.started": "2025-05-16T21:51:49.108319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_add = pd.read_csv('/kaggle/input/ellipse-combined/asap_1_2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:51:51.257221Z",
     "iopub.status.busy": "2025-05-16T21:51:51.256500Z",
     "iopub.status.idle": "2025-05-16T21:51:51.265930Z",
     "shell.execute_reply": "2025-05-16T21:51:51.265272Z",
     "shell.execute_reply.started": "2025-05-16T21:51:51.257200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_add = df_add[df_add['essay_set'].isin([1, 2])].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:51:52.003361Z",
     "iopub.status.busy": "2025-05-16T21:51:52.002725Z",
     "iopub.status.idle": "2025-05-16T21:51:52.006830Z",
     "shell.execute_reply": "2025-05-16T21:51:52.006124Z",
     "shell.execute_reply.started": "2025-05-16T21:51:52.003337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_add = df_add['essay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:51:53.002855Z",
     "iopub.status.busy": "2025-05-16T21:51:53.002151Z",
     "iopub.status.idle": "2025-05-16T21:51:53.006208Z",
     "shell.execute_reply": "2025-05-16T21:51:53.005527Z",
     "shell.execute_reply.started": "2025-05-16T21:51:53.002833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_add = df_add.rename('full_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:51:53.335871Z",
     "iopub.status.busy": "2025-05-16T21:51:53.335609Z",
     "iopub.status.idle": "2025-05-16T21:51:53.340283Z",
     "shell.execute_reply": "2025-05-16T21:51:53.339595Z",
     "shell.execute_reply.started": "2025-05-16T21:51:53.335851Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pseudo = pd.concat([train_df['full_text'], df_add], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:51:55.231642Z",
     "iopub.status.busy": "2025-05-16T21:51:55.231150Z",
     "iopub.status.idle": "2025-05-16T21:51:55.236593Z",
     "shell.execute_reply": "2025-05-16T21:51:55.235899Z",
     "shell.execute_reply.started": "2025-05-16T21:51:55.231619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row.tolist()\n",
    "\n",
    "\n",
    "        inputs = self.tokenizer(text,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                                max_length=512)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:51:57.070972Z",
     "iopub.status.busy": "2025-05-16T21:51:57.070343Z",
     "iopub.status.idle": "2025-05-16T21:51:57.074747Z",
     "shell.execute_reply": "2025-05-16T21:51:57.073961Z",
     "shell.execute_reply.started": "2025-05-16T21:51:57.070948Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "add_dataset = UnlabeledDataset(pseudo, tokenizer)\n",
    "add_loader = DataLoader(add_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:51:58.047776Z",
     "iopub.status.busy": "2025-05-16T21:51:58.047279Z",
     "iopub.status.idle": "2025-05-16T21:55:07.840575Z",
     "shell.execute_reply": "2025-05-16T21:55:07.839886Z",
     "shell.execute_reply.started": "2025-05-16T21:51:58.047756Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 949/949 [03:09<00:00,  5.00it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "loop = tqdm(add_loader)\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        preds = model(input_ids, attention_mask)\n",
    "        all_preds.append(preds)\n",
    "all_preds = torch.cat(all_preds, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:55:51.429480Z",
     "iopub.status.busy": "2025-05-16T21:55:51.428806Z",
     "iopub.status.idle": "2025-05-16T21:55:51.432829Z",
     "shell.execute_reply": "2025-05-16T21:55:51.432202Z",
     "shell.execute_reply.started": "2025-05-16T21:55:51.429460Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns=[\"Overall\",\n",
    "         \"Cohesion\",\n",
    "         \"Syntax\",\n",
    "         \"Vocabulary\",\n",
    "         \"Phraseology\",\n",
    "         \"Grammar\",\n",
    "         \"Conventions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:55:52.179795Z",
     "iopub.status.busy": "2025-05-16T21:55:52.179532Z",
     "iopub.status.idle": "2025-05-16T21:55:52.199743Z",
     "shell.execute_reply": "2025-05-16T21:55:52.198970Z",
     "shell.execute_reply.started": "2025-05-16T21:55:52.179775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pseudo_df = pd.DataFrame(data=list(zip(*all_preds.T)), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:55:52.911462Z",
     "iopub.status.busy": "2025-05-16T21:55:52.910768Z",
     "iopub.status.idle": "2025-05-16T21:55:52.915831Z",
     "shell.execute_reply": "2025-05-16T21:55:52.915129Z",
     "shell.execute_reply.started": "2025-05-16T21:55:52.911439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pseudo_df['full_text'] = pseudo.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:55:55.768253Z",
     "iopub.status.busy": "2025-05-16T21:55:55.767583Z",
     "iopub.status.idle": "2025-05-16T21:56:13.418909Z",
     "shell.execute_reply": "2025-05-16T21:56:13.418347Z",
     "shell.execute_reply.started": "2025-05-16T21:55:55.768230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/2701176831.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.6066778600215912' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  pseudo_df.loc[i, col] = (pseudo_df.loc[i, col] + train_df.reset_index().loc[i, col]) / 2\n",
      "/tmp/ipykernel_35/2701176831.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.6528847515583038' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  pseudo_df.loc[i, col] = (pseudo_df.loc[i, col] + train_df.reset_index().loc[i, col]) / 2\n",
      "/tmp/ipykernel_35/2701176831.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.5361486375331879' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  pseudo_df.loc[i, col] = (pseudo_df.loc[i, col] + train_df.reset_index().loc[i, col]) / 2\n",
      "/tmp/ipykernel_35/2701176831.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.6628443896770477' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  pseudo_df.loc[i, col] = (pseudo_df.loc[i, col] + train_df.reset_index().loc[i, col]) / 2\n",
      "/tmp/ipykernel_35/2701176831.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.6367725431919098' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  pseudo_df.loc[i, col] = (pseudo_df.loc[i, col] + train_df.reset_index().loc[i, col]) / 2\n",
      "/tmp/ipykernel_35/2701176831.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.5985667407512665' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  pseudo_df.loc[i, col] = (pseudo_df.loc[i, col] + train_df.reset_index().loc[i, col]) / 2\n",
      "/tmp/ipykernel_35/2701176831.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.9058404266834259' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  pseudo_df.loc[i, col] = (pseudo_df.loc[i, col] + train_df.reset_index().loc[i, col]) / 2\n"
     ]
    }
   ],
   "source": [
    "for col in columns:\n",
    "    for i in range(len(train_df)):\n",
    "        pseudo_df.loc[i, col] = (pseudo_df.loc[i, col] + train_df.reset_index().loc[i, col]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:56:49.191576Z",
     "iopub.status.busy": "2025-05-16T21:56:49.190902Z",
     "iopub.status.idle": "2025-05-16T21:56:49.196700Z",
     "shell.execute_reply": "2025-05-16T21:56:49.195927Z",
     "shell.execute_reply.started": "2025-05-16T21:56:49.191548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PseudoDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.df = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = row[\"full_text\"].to_list()\n",
    "\n",
    "\n",
    "        inputs = self.tokenizer(text,\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\",\n",
    "                                max_length=512)\n",
    "\n",
    "        scores = torch.tensor(row[columns].values, dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"scores\": scores\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:56:50.904736Z",
     "iopub.status.busy": "2025-05-16T21:56:50.904492Z",
     "iopub.status.idle": "2025-05-16T21:56:50.908847Z",
     "shell.execute_reply": "2025-05-16T21:56:50.908140Z",
     "shell.execute_reply.started": "2025-05-16T21:56:50.904720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "combined_dataset = PseudoDataset(pseudo_df, tokenizer)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:57:01.053011Z",
     "iopub.status.busy": "2025-05-16T21:57:01.052503Z",
     "iopub.status.idle": "2025-05-16T21:57:02.860272Z",
     "shell.execute_reply": "2025-05-16T21:57:02.859577Z",
     "shell.execute_reply.started": "2025-05-16T21:57:01.052990Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_2 = DebertaNHeads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:57:03.007846Z",
     "iopub.status.busy": "2025-05-16T21:57:03.007575Z",
     "iopub.status.idle": "2025-05-16T21:57:03.012806Z",
     "shell.execute_reply": "2025-05-16T21:57:03.012134Z",
     "shell.execute_reply.started": "2025-05-16T21:57:03.007825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for layer in model_2.deberta.encoder.layer[:9]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "for param in model_2.deberta.embeddings.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:57:05.070044Z",
     "iopub.status.busy": "2025-05-16T21:57:05.069592Z",
     "iopub.status.idle": "2025-05-16T21:57:05.281171Z",
     "shell.execute_reply": "2025-05-16T21:57:05.280389Z",
     "shell.execute_reply.started": "2025-05-16T21:57:05.070021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_training_steps = len(combined_loader) * 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_2.to(device)\n",
    "optimizer = optim.AdamW(model_2.parameters(), lr=4e-5)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T21:57:07.042029Z",
     "iopub.status.busy": "2025-05-16T21:57:07.041526Z",
     "iopub.status.idle": "2025-05-16T23:19:03.522942Z",
     "shell.execute_reply": "2025-05-16T23:19:03.522292Z",
     "shell.execute_reply.started": "2025-05-16T21:57:07.042009Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.2437857589321937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7224\n",
      "QWK for Cohesion: 0.6188\n",
      "QWK for Syntax: 0.6876\n",
      "QWK for Vocabulary: 0.6557\n",
      "QWK for Phraseology: 0.6687\n",
      "QWK for Grammar: 0.7036\n",
      "QWK for Conventions: 0.7106\n",
      "New best QWK: 0.6811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.208767759169409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7317\n",
      "QWK for Cohesion: 0.6244\n",
      "QWK for Syntax: 0.6190\n",
      "QWK for Vocabulary: 0.5825\n",
      "QWK for Phraseology: 0.6111\n",
      "QWK for Grammar: 0.7276\n",
      "QWK for Conventions: 0.6742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.1929958545400234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7689\n",
      "QWK for Cohesion: 0.6497\n",
      "QWK for Syntax: 0.7076\n",
      "QWK for Vocabulary: 0.6877\n",
      "QWK for Phraseology: 0.6967\n",
      "QWK for Grammar: 0.7363\n",
      "QWK for Conventions: 0.7160\n",
      "New best QWK: 0.7090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.18668466211555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7147\n",
      "QWK for Cohesion: 0.6303\n",
      "QWK for Syntax: 0.6885\n",
      "QWK for Vocabulary: 0.6662\n",
      "QWK for Phraseology: 0.6877\n",
      "QWK for Grammar: 0.7169\n",
      "QWK for Conventions: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.1835498747949353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7604\n",
      "QWK for Cohesion: 0.6520\n",
      "QWK for Syntax: 0.6987\n",
      "QWK for Vocabulary: 0.6651\n",
      "QWK for Phraseology: 0.6993\n",
      "QWK for Grammar: 0.7319\n",
      "QWK for Conventions: 0.7107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.181679682936259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7681\n",
      "QWK for Cohesion: 0.6436\n",
      "QWK for Syntax: 0.6958\n",
      "QWK for Vocabulary: 0.6657\n",
      "QWK for Phraseology: 0.6904\n",
      "QWK for Grammar: 0.7252\n",
      "QWK for Conventions: 0.7110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.1805262701834984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7586\n",
      "QWK for Cohesion: 0.6411\n",
      "QWK for Syntax: 0.6838\n",
      "QWK for Vocabulary: 0.6552\n",
      "QWK for Phraseology: 0.6869\n",
      "QWK for Grammar: 0.7219\n",
      "QWK for Conventions: 0.7041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.1796782647302289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7589\n",
      "QWK for Cohesion: 0.6491\n",
      "QWK for Syntax: 0.6935\n",
      "QWK for Vocabulary: 0.6685\n",
      "QWK for Phraseology: 0.6996\n",
      "QWK for Grammar: 0.7298\n",
      "QWK for Conventions: 0.7127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.179052935507959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 163/163 [00:32<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7606\n",
      "QWK for Cohesion: 0.6546\n",
      "QWK for Syntax: 0.6880\n",
      "QWK for Vocabulary: 0.6713\n",
      "QWK for Phraseology: 0.6917\n",
      "QWK for Grammar: 0.7311\n",
      "QWK for Conventions: 0.7083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 949/949 [07:38<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss_per_epoch = 1.1787844161073604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 163/163 [00:32<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7622\n",
      "QWK for Cohesion: 0.6537\n",
      "QWK for Syntax: 0.6903\n",
      "QWK for Vocabulary: 0.6717\n",
      "QWK for Phraseology: 0.6936\n",
      "QWK for Grammar: 0.7299\n",
      "QWK for Conventions: 0.7135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "qwk_scores = []\n",
    "best_qwk = -float('inf')\n",
    "\n",
    "model_2.train()\n",
    "for epoch in range(10):\n",
    "    loss_per_epoch = 0\n",
    "    loop = tqdm(combined_loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        reg_target = batch[\"scores\"].to(device)\n",
    "        reg_preds = model_2(input_ids, attention_mask)\n",
    "        loss = 0\n",
    "        for i in range(7):\n",
    "            loss += combined_loss(reg_preds[:, i], reg_target[:, i])\n",
    "        loss /= 7\n",
    "        loss_per_epoch += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model_2.parameters(), max_norm=10)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_loss = loss_per_epoch / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f'Loss_per_epoch = {avg_loss}')\n",
    "\n",
    "    model_2.eval()\n",
    "    loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    with torch.no_grad():\n",
    "        for batch in loop:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            reg_target = batch[\"scores\"].to(device)\n",
    "            reg_preds = model_2(input_ids, attention_mask)\n",
    "            all_preds.append(reg_preds)\n",
    "            all_targets.append(reg_target)\n",
    "    all_preds = torch.cat(all_preds, dim=0).detach().cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets, dim=0).detach().cpu().numpy()\n",
    "    qwk = evaluate_qwk_for_aspects(all_preds, all_targets, aspects=['Overall', 'Cohesion', 'Syntax', 'Vocabulary', 'Phraseology', 'Grammar', 'Conventions'])\n",
    "    qwk_scores.append(qwk)\n",
    "    avg_qwk = sum(qwk.values()) / len(qwk)\n",
    "    if avg_qwk - best_qwk > 1e-6:\n",
    "        best_qwk = avg_qwk\n",
    "        print(f\"New best QWK: {best_qwk:.4f}\")\n",
    "        torch.save(model_2.state_dict(), \"/kaggle/working/best_second_model.pth\")\n",
    "    model_2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T23:19:06.901330Z",
     "iopub.status.busy": "2025-05-16T23:19:06.900997Z",
     "iopub.status.idle": "2025-05-16T23:19:09.844226Z",
     "shell.execute_reply": "2025-05-16T23:19:09.843611Z",
     "shell.execute_reply.started": "2025-05-16T23:19:06.901291Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaNHeads(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128002, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (regression_heads): ModuleList(\n",
       "    (0-6): 7 x Linear(in_features=768, out_features=1, bias=True)\n",
       "  )\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = DebertaNHeads()\n",
    "model_2.load_state_dict(torch.load('/kaggle/working/best_second_model.pth', weights_only=True))\n",
    "model_2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-16T23:19:12.163935Z",
     "iopub.status.busy": "2025-05-16T23:19:12.163308Z",
     "iopub.status.idle": "2025-05-16T23:19:44.666223Z",
     "shell.execute_reply": "2025-05-16T23:19:44.665517Z",
     "shell.execute_reply.started": "2025-05-16T23:19:12.163912Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [00:32<00:00,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QWK for Overall: 0.7459\n",
      "QWK for Cohesion: 0.6401\n",
      "QWK for Syntax: 0.6703\n",
      "QWK for Vocabulary: 0.6442\n",
      "QWK for Phraseology: 0.6478\n",
      "QWK for Grammar: 0.7074\n",
      "QWK for Conventions: 0.6981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_2.eval()\n",
    "loop = tqdm(test_loader)\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in loop:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        target = batch[\"scores\"].to(device)\n",
    "        preds = model_2(input_ids, attention_mask)\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(target)\n",
    "all_preds = torch.cat(all_preds, dim=0).detach().cpu().numpy()\n",
    "all_targets = torch.cat(all_targets, dim=0).detach().cpu().numpy()\n",
    "qwk = evaluate_qwk_for_aspects(all_preds, all_targets, aspects=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7439072,
     "sourceId": 11840206,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7110184,
     "sourceId": 11496551,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
