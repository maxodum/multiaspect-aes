{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11378321,"sourceType":"datasetVersion","datasetId":7123942}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:38:15.939696Z","iopub.execute_input":"2025-04-21T13:38:15.939892Z","iopub.status.idle":"2025-04-21T13:38:15.943395Z","shell.execute_reply.started":"2025-04-21T13:38:15.939875Z","shell.execute_reply":"2025-04-21T13:38:15.942619Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv('../input/data.csv', index_col=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:38:15.944231Z","iopub.execute_input":"2025-04-21T13:38:15.944758Z","iopub.status.idle":"2025-04-21T13:38:16.169457Z","shell.execute_reply.started":"2025-04-21T13:38:15.944734Z","shell.execute_reply":"2025-04-21T13:38:16.168865Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:38:16.866954Z","iopub.execute_input":"2025-04-21T13:38:16.868061Z","iopub.status.idle":"2025-04-21T13:38:17.351567Z","shell.execute_reply.started":"2025-04-21T13:38:16.868003Z","shell.execute_reply":"2025-04-21T13:38:17.350909Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:38:18.774603Z","iopub.execute_input":"2025-04-21T13:38:18.775070Z","iopub.status.idle":"2025-04-21T13:38:18.783556Z","shell.execute_reply.started":"2025-04-21T13:38:18.775011Z","shell.execute_reply":"2025-04-21T13:38:18.782718Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:38:20.694447Z","iopub.execute_input":"2025-04-21T13:38:20.694746Z","iopub.status.idle":"2025-04-21T13:38:22.524848Z","shell.execute_reply.started":"2025-04-21T13:38:20.694723Z","shell.execute_reply":"2025-04-21T13:38:22.524276Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class EllipseDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_input_length=512, max_target_length=64):\n        self.df = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        self.max_target_length = max_target_length\n        self.score_keys = ['Overall', 'Cohesion', 'Syntax', 'Vocabulary', 'Phraseology', 'Grammar', 'Conventions']\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        essay_text = row[\"full_text\"]\n        input_text = f\"score essay: {essay_text}\"\n        \n        target_text = \", \".join([f\"{key.lower()}: {row[key]:.1f}\" for key in self.score_keys])\n\n        input_enc = self.tokenizer(\n            input_text,\n            max_length=self.max_input_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        target_enc = self.tokenizer(\n            target_text,\n            max_length=self.max_target_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n            \"labels\": target_enc[\"input_ids\"].squeeze(0),\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:38:24.521284Z","iopub.execute_input":"2025-04-21T13:38:24.522070Z","iopub.status.idle":"2025-04-21T13:38:24.528399Z","shell.execute_reply.started":"2025-04-21T13:38:24.522036Z","shell.execute_reply":"2025-04-21T13:38:24.527741Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from transformers import T5Tokenizer\ntokenizer = T5Tokenizer.from_pretrained('t5-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:38:31.170875Z","iopub.execute_input":"2025-04-21T13:38:31.171199Z","iopub.status.idle":"2025-04-21T13:38:33.866434Z","shell.execute_reply.started":"2025-04-21T13:38:31.171175Z","shell.execute_reply":"2025-04-21T13:38:33.865787Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"train_dataset = EllipseDataset(train_df, tokenizer)\ntest_dataset = EllipseDataset(test_df, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:38:40.266883Z","iopub.execute_input":"2025-04-21T13:38:40.267866Z","iopub.status.idle":"2025-04-21T13:38:40.272593Z","shell.execute_reply.started":"2025-04-21T13:38:40.267827Z","shell.execute_reply":"2025-04-21T13:38:40.271684Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback, AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained('t5-base')\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    save_strategy=\"no\",\n    eval_strategy=\"steps\",\n    eval_steps=5000,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=15,\n    report_to=\"none\",\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:38:47.177788Z","iopub.execute_input":"2025-04-21T13:38:47.178115Z","iopub.status.idle":"2025-04-21T15:55:26.215661Z","shell.execute_reply.started":"2025-04-21T13:38:47.178090Z","shell.execute_reply":"2025-04-21T15:55:26.214902Z"}},"outputs":[{"name":"stderr","text":"2025-04-21 13:38:49.311522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745242729.334638    1425 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745242729.341691    1425 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/tmp/ipykernel_1425/1815218009.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9735' max='9735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9735/9735 2:16:30, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5000</td>\n      <td>0.120100</td>\n      <td>0.128246</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9735, training_loss=0.12955107764825494, metrics={'train_runtime': 8192.3863, 'train_samples_per_second': 9.494, 'train_steps_per_second': 1.188, 'total_flos': 4.7361699938304e+16, 'train_loss': 0.12955107764825494, 'epoch': 15.0})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T15:55:32.751844Z","iopub.execute_input":"2025-04-21T15:55:32.752977Z","iopub.status.idle":"2025-04-21T15:55:32.756610Z","shell.execute_reply.started":"2025-04-21T15:55:32.752948Z","shell.execute_reply":"2025-04-21T15:55:32.755713Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nloader = DataLoader(test_dataset, batch_size=1)\n\npreds = []\nlabels = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(loader):\n        input_ids = batch[\"input_ids\"].to(model.device)\n        attention_mask = batch[\"attention_mask\"].to(model.device)\n        label_ids = batch[\"labels\"].to(model.device)\n\n        output = model.generate(input_ids, attention_mask=attention_mask, max_length=64)\n        preds.append(tokenizer.decode(output[0], skip_special_tokens=True))\n        labels.append(tokenizer.decode(label_ids[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T15:55:36.049729Z","iopub.execute_input":"2025-04-21T15:55:36.049982Z","iopub.status.idle":"2025-04-21T16:10:53.338342Z","shell.execute_reply.started":"2025-04-21T15:55:36.049964Z","shell.execute_reply":"2025-04-21T16:10:53.337530Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1297/1297 [15:17<00:00,  1.41it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\nimport numpy as np\nimport re\n\ndef parse_scores(text):\n    pattern = r\"(\\w+):\\s*([\\d.]+)\"\n    found = re.findall(pattern, text.lower())\n    score_dict = {key: float(value) for key, value in found}\n    return score_dict\n\nscore_keys = ['overall', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\npredictions = [parse_scores(t) for t in preds]\nground_truths = [parse_scores(t) for t in labels]\n\ny_min = 1\ny_max = 5\nfor key in score_keys:\n    pred_vals = [pred.get(key, 3.0) for pred in predictions]\n    true_vals = [gt.get(key, 3.0) for gt in ground_truths]\n\n    y_pred_int = np.rint(2 * np.array(pred_vals)).astype(int)\n    y_true_int = np.rint(2 * np.array(true_vals)).astype(int)\n\n    qwk = cohen_kappa_score(y_pred_int, y_true_int, weights='quadratic')\n    print(f'QWK_{key.title()} = {qwk:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T16:10:58.965351Z","iopub.execute_input":"2025-04-21T16:10:58.965693Z","iopub.status.idle":"2025-04-21T16:10:59.027552Z","shell.execute_reply.started":"2025-04-21T16:10:58.965669Z","shell.execute_reply":"2025-04-21T16:10:59.026792Z"}},"outputs":[{"name":"stdout","text":"QWK_Overall = 0.6865\nQWK_Cohesion = 0.5897\nQWK_Syntax = 0.6216\nQWK_Vocabulary = 0.6129\nQWK_Phraseology = 0.6298\nQWK_Grammar = 0.6583\nQWK_Conventions = 0.6344\n","output_type":"stream"}],"execution_count":13}]}