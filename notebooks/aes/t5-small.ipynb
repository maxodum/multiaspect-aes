{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11378321,"sourceType":"datasetVersion","datasetId":7123942}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:32:47.250056Z","iopub.execute_input":"2025-04-21T08:32:47.250349Z","iopub.status.idle":"2025-04-21T08:32:48.992480Z","shell.execute_reply.started":"2025-04-21T08:32:47.250321Z","shell.execute_reply":"2025-04-21T08:32:48.991638Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/essays/data.csv', index_col=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:32:52.538885Z","iopub.execute_input":"2025-04-21T08:32:52.539437Z","iopub.status.idle":"2025-04-21T08:32:52.991688Z","shell.execute_reply.started":"2025-04-21T08:32:52.539410Z","shell.execute_reply":"2025-04-21T08:32:52.991110Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:32:55.526795Z","iopub.execute_input":"2025-04-21T08:32:55.527047Z","iopub.status.idle":"2025-04-21T08:32:56.190266Z","shell.execute_reply.started":"2025-04-21T08:32:55.527027Z","shell.execute_reply":"2025-04-21T08:32:56.189429Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:32:57.988831Z","iopub.execute_input":"2025-04-21T08:32:57.989292Z","iopub.status.idle":"2025-04-21T08:32:58.000839Z","shell.execute_reply.started":"2025-04-21T08:32:57.989267Z","shell.execute_reply":"2025-04-21T08:32:57.999976Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:32:59.304689Z","iopub.execute_input":"2025-04-21T08:32:59.304973Z","iopub.status.idle":"2025-04-21T08:33:03.878598Z","shell.execute_reply.started":"2025-04-21T08:32:59.304935Z","shell.execute_reply":"2025-04-21T08:33:03.877961Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class EllipseDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_input_length=512, max_target_length=64):\n        self.df = dataframe.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_input_length = max_input_length\n        self.max_target_length = max_target_length\n        self.score_keys = ['Overall', 'Cohesion', 'Syntax', 'Vocabulary', 'Phraseology', 'Grammar', 'Conventions']\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n\n        essay_text = row[\"full_text\"]\n        input_text = f\"score essay: {essay_text}\"\n\n        target_text = \", \".join([f\"{key.lower()}: {row[key]:.1f}\" for key in self.score_keys])\n\n        input_enc = self.tokenizer(\n            input_text,\n            max_length=self.max_input_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        target_enc = self.tokenizer(\n            target_text,\n            max_length=self.max_target_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": input_enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": input_enc[\"attention_mask\"].squeeze(0),\n            \"labels\": target_enc[\"input_ids\"].squeeze(0),\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:33:06.063720Z","iopub.execute_input":"2025-04-21T08:33:06.064557Z","iopub.status.idle":"2025-04-21T08:33:06.071151Z","shell.execute_reply.started":"2025-04-21T08:33:06.064535Z","shell.execute_reply":"2025-04-21T08:33:06.070301Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import T5Tokenizer\ntokenizer = T5Tokenizer.from_pretrained('t5-small')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:33:10.154506Z","iopub.execute_input":"2025-04-21T08:33:10.154767Z","iopub.status.idle":"2025-04-21T08:33:13.465757Z","shell.execute_reply.started":"2025-04-21T08:33:10.154751Z","shell.execute_reply":"2025-04-21T08:33:13.464929Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf9dbb44eb554e18906415bc5a317e54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"562f91b48ed74edbaa3d4628b38f1291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40c966e22ff94ce58bea51063f61bf2e"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"train_dataset = EllipseDataset(train_df, tokenizer)\ntest_dataset = EllipseDataset(test_df, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:33:16.807433Z","iopub.execute_input":"2025-04-21T08:33:16.807844Z","iopub.status.idle":"2025-04-21T08:33:16.812971Z","shell.execute_reply.started":"2025-04-21T08:33:16.807822Z","shell.execute_reply":"2025-04-21T08:33:16.812151Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback, AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    eval_steps=5000,                      # Evaluate every 5000 steps\n    per_device_train_batch_size=4,        # Batch size of 4\n    per_device_eval_batch_size=4,\n    num_train_epochs=15,                  # Total epochs: 15\n    report_to=\"none\",                     # Disable W&B/Hub logging unless needed\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    tokenizer=tokenizer,\n    # callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T08:33:23.741661Z","iopub.execute_input":"2025-04-21T08:33:23.741939Z","iopub.status.idle":"2025-04-21T09:16:03.003459Z","shell.execute_reply.started":"2025-04-21T08:33:23.741920Z","shell.execute_reply":"2025-04-21T09:16:03.002790Z"}},"outputs":[{"name":"stderr","text":"2025-04-21 08:33:30.511303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745224410.735204      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745224410.796498      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0619fd8350048089485b66c0e8aa698"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de1f90b01cd54ea8b779d24b42c609fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aac37757da04701a250b0869ec87f79"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_31/2557507440.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9735' max='9735' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9735/9735 42:14, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.620800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.156800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.145800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.142500</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.139200</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.137500</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.135000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.135000</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.133800</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.132900</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.132100</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.131400</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.130200</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.131000</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.129600</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.129300</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.129500</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.128800</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.128900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9735, training_loss=0.15978561483656498, metrics={'train_runtime': 2536.548, 'train_samples_per_second': 30.662, 'train_steps_per_second': 3.838, 'total_flos': 1.05262086094848e+16, 'train_loss': 0.15978561483656498, 'epoch': 15.0})"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:40:19.606488Z","iopub.execute_input":"2025-04-21T09:40:19.607485Z","iopub.status.idle":"2025-04-21T09:40:19.610943Z","shell.execute_reply.started":"2025-04-21T09:40:19.607459Z","shell.execute_reply":"2025-04-21T09:40:19.610288Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nloader = DataLoader(test_dataset, batch_size=1)\n\npreds = []\nlabels = []\nmodel.eval()\nwith torch.no_grad():\n    for batch in tqdm(loader):\n        input_ids = batch[\"input_ids\"].to(model.device)\n        attention_mask = batch[\"attention_mask\"].to(model.device)\n        label_ids = batch[\"labels\"].to(model.device)\n\n        output = model.generate(input_ids, attention_mask=attention_mask, max_length=64)\n        preds.append(tokenizer.decode(output[0], skip_special_tokens=True))\n        labels.append(tokenizer.decode(label_ids[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T09:40:22.612187Z","iopub.execute_input":"2025-04-21T09:40:22.612460Z","iopub.status.idle":"2025-04-21T09:48:27.969930Z","shell.execute_reply.started":"2025-04-21T09:40:22.612439Z","shell.execute_reply":"2025-04-21T09:48:27.969169Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 1297/1297 [08:05<00:00,  2.67it/s]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from sklearn.metrics import cohen_kappa_score\nimport numpy as np\nimport re\n\ndef parse_scores(text):\n    pattern = r\"(\\w+):\\s*([\\d.]+)\"\n    found = re.findall(pattern, text.lower())\n    score_dict = {key: float(value) for key, value in found}\n    return score_dict\n\nscore_keys = ['overall', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\npredictions = [parse_scores(t) for t in preds]\nground_truths = [parse_scores(t) for t in labels]\n\ny_min = 1\ny_max = 5\nfor key in score_keys:\n    pred_vals = [pred.get(key, 3.0) for pred in predictions]  # fallback value if missing\n    true_vals = [gt.get(key, 3.0) for gt in ground_truths]\n\n    y_pred_int = np.rint(2 * np.array(pred_vals)).astype(int)\n    y_true_int = np.rint(2 * np.array(true_vals)).astype(int)\n\n    qwk = cohen_kappa_score(y_pred_int, y_true_int, weights='quadratic')\n    print(f'QWK_{key.title()} = {qwk:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T10:00:36.091375Z","iopub.execute_input":"2025-04-21T10:00:36.091655Z","iopub.status.idle":"2025-04-21T10:00:36.131498Z","shell.execute_reply.started":"2025-04-21T10:00:36.091637Z","shell.execute_reply":"2025-04-21T10:00:36.130653Z"}},"outputs":[{"name":"stdout","text":"QWK_Overall = 0.6937\nQWK_Cohesion = 0.5941\nQWK_Syntax = 0.6197\nQWK_Vocabulary = 0.6146\nQWK_Phraseology = 0.6399\nQWK_Grammar = 0.6161\nQWK_Conventions = 0.6307\n","output_type":"stream"}],"execution_count":14}]}